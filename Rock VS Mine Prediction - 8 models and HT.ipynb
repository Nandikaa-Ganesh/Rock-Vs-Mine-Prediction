{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYSLvRgB3Sel"
   },
   "source": [
    "## Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cbE3ZjDb23el"
   },
   "outputs": [],
   "source": [
    "import numpy as np #for computations\n",
    "import pandas as pd #for data storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCLGacZR4UZx"
   },
   "source": [
    "## Data Collection and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7ymxgj2i3RwO"
   },
   "outputs": [],
   "source": [
    "#loading the dataset to a pandas Dataframe\n",
    "sonar_data = pd.read_csv('Datasets/sonar data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "I5iWxSnM42fl",
    "outputId": "1b2221d0-dd71-40b2-c7fc-272017ea53f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "sonar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN_FI_eN48V_",
    "outputId": "5d4d105c-657c-4eec-df90-e2f4a0cbf837"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows and columns\n",
    "sonar_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60 features and 208 data points - last column represents whether it is rock or mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "q6A1r9J-5aOJ",
    "outputId": "9efbb9de-570b-4d9f-a92a-d6835f3acb2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9   ...          50  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \n",
       "std      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \n",
       "min      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n",
       "25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n",
       "50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n",
       "75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \n",
       "max      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n",
       "\n",
       "               51          52          53          54          55          56  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \n",
       "std      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \n",
       "min      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n",
       "25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n",
       "50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n",
       "75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \n",
       "max      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n",
       "\n",
       "               57          58          59  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007949    0.007941    0.006507  \n",
       "std      0.006470    0.006181    0.005031  \n",
       "min      0.000300    0.000100    0.000600  \n",
       "25%      0.003600    0.003675    0.003100  \n",
       "50%      0.005800    0.006400    0.005300  \n",
       "75%      0.010350    0.010325    0.008525  \n",
       "max      0.044000    0.036400    0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.describe()  #describe --> statistical measures of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFlxfDyk5o00",
    "outputId": "4ca928a6-de7b-439e-8a94-bc89a4449189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60\n",
       "M    111\n",
       "R     97\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data[60].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     177\n",
       "1     182\n",
       "2     190\n",
       "3     181\n",
       "4     193\n",
       "     ... \n",
       "56    121\n",
       "57    124\n",
       "58    119\n",
       "59    109\n",
       "60      2\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = sonar_data.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "203    False\n",
       "204    False\n",
       "205    False\n",
       "206    False\n",
       "207    False\n",
       "Length: 208, dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6RDFTc26aBI"
   },
   "source": [
    "M --> Mine\n",
    "\n",
    "R --> Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "Uis1XlFs6M09",
    "outputId": "e7c33de6-9384-44d6-a66d-e60cbf0d8662"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.034989</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.050720</td>\n",
       "      <td>0.064768</td>\n",
       "      <td>0.086715</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>0.128359</td>\n",
       "      <td>0.149832</td>\n",
       "      <td>0.213492</td>\n",
       "      <td>0.251022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019352</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.011643</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>0.006930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.022498</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035951</td>\n",
       "      <td>0.041447</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.096224</td>\n",
       "      <td>0.114180</td>\n",
       "      <td>0.117596</td>\n",
       "      <td>0.137392</td>\n",
       "      <td>0.159325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012311</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.006024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "60                                                                         \n",
       "M   0.034989  0.045544  0.050720  0.064768  0.086715  0.111864  0.128359   \n",
       "R   0.022498  0.030303  0.035951  0.041447  0.062028  0.096224  0.114180   \n",
       "\n",
       "          7         8         9   ...        50        51        52        53  \\\n",
       "60                                ...                                           \n",
       "M   0.149832  0.213492  0.251022  ...  0.019352  0.016014  0.011643  0.012185   \n",
       "R   0.117596  0.137392  0.159325  ...  0.012311  0.010453  0.009640  0.009518   \n",
       "\n",
       "          54        55        56        57        58        59  \n",
       "60                                                              \n",
       "M   0.009923  0.008914  0.007825  0.009060  0.008695  0.006930  \n",
       "R   0.008567  0.007430  0.007814  0.006677  0.007078  0.006024  \n",
       "\n",
       "[2 rows x 60 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.groupby(60).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qRShuFc46jLd"
   },
   "outputs": [],
   "source": [
    "# separating data and Labels\n",
    "X = sonar_data.drop(columns=60, axis=1)\n",
    "Y = sonar_data[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkRRrxIe7D7l",
    "outputId": "4dbba519-c5cf-4721-dafa-3c2c31439206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2       3       4       5       6       7       8   \\\n",
      "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
      "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
      "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
      "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
      "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
      "\n",
      "         9   ...      50      51      52      53      54      55      56  \\\n",
      "0    0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n",
      "1    0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n",
      "2    0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n",
      "3    0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n",
      "4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n",
      "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n",
      "204  0.2154  ...  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034   \n",
      "205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n",
      "206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n",
      "207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n",
      "\n",
      "         57      58      59  \n",
      "0    0.0084  0.0090  0.0032  \n",
      "1    0.0049  0.0052  0.0044  \n",
      "2    0.0164  0.0095  0.0078  \n",
      "3    0.0044  0.0040  0.0117  \n",
      "4    0.0048  0.0107  0.0094  \n",
      "..      ...     ...     ...  \n",
      "203  0.0115  0.0193  0.0157  \n",
      "204  0.0032  0.0062  0.0067  \n",
      "205  0.0138  0.0077  0.0031  \n",
      "206  0.0079  0.0036  0.0048  \n",
      "207  0.0036  0.0061  0.0115  \n",
      "\n",
      "[208 rows x 60 columns]\n",
      "0      R\n",
      "1      R\n",
      "2      R\n",
      "3      R\n",
      "4      R\n",
      "      ..\n",
      "203    M\n",
      "204    M\n",
      "205    M\n",
      "206    M\n",
      "207    M\n",
      "Name: 60, Length: 208, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735896</td>\n",
       "      <td>0.571537</td>\n",
       "      <td>0.491438</td>\n",
       "      <td>0.344797</td>\n",
       "      <td>0.238921</td>\n",
       "      <td>0.260815</td>\n",
       "      <td>0.355523</td>\n",
       "      <td>0.353420</td>\n",
       "      <td>0.318276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254450</td>\n",
       "      <td>0.355299</td>\n",
       "      <td>0.311729</td>\n",
       "      <td>0.322299</td>\n",
       "      <td>0.312067</td>\n",
       "      <td>0.220642</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.368132</td>\n",
       "      <td>0.357116</td>\n",
       "      <td>0.347078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.735896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779916</td>\n",
       "      <td>0.606684</td>\n",
       "      <td>0.419669</td>\n",
       "      <td>0.332329</td>\n",
       "      <td>0.279040</td>\n",
       "      <td>0.334615</td>\n",
       "      <td>0.316733</td>\n",
       "      <td>0.270782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320538</td>\n",
       "      <td>0.434548</td>\n",
       "      <td>0.346076</td>\n",
       "      <td>0.383960</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>0.262263</td>\n",
       "      <td>0.280341</td>\n",
       "      <td>0.353042</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.358761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571537</td>\n",
       "      <td>0.779916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781786</td>\n",
       "      <td>0.546141</td>\n",
       "      <td>0.346275</td>\n",
       "      <td>0.190434</td>\n",
       "      <td>0.237884</td>\n",
       "      <td>0.252691</td>\n",
       "      <td>0.219637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238110</td>\n",
       "      <td>0.394076</td>\n",
       "      <td>0.332914</td>\n",
       "      <td>0.367186</td>\n",
       "      <td>0.289731</td>\n",
       "      <td>0.287661</td>\n",
       "      <td>0.380819</td>\n",
       "      <td>0.334108</td>\n",
       "      <td>0.425047</td>\n",
       "      <td>0.373948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491438</td>\n",
       "      <td>0.606684</td>\n",
       "      <td>0.781786</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726943</td>\n",
       "      <td>0.352805</td>\n",
       "      <td>0.246440</td>\n",
       "      <td>0.246742</td>\n",
       "      <td>0.247078</td>\n",
       "      <td>0.237769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174676</td>\n",
       "      <td>0.374651</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.334211</td>\n",
       "      <td>0.284955</td>\n",
       "      <td>0.280938</td>\n",
       "      <td>0.340254</td>\n",
       "      <td>0.344865</td>\n",
       "      <td>0.420266</td>\n",
       "      <td>0.400626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.344797</td>\n",
       "      <td>0.419669</td>\n",
       "      <td>0.546141</td>\n",
       "      <td>0.726943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597053</td>\n",
       "      <td>0.335422</td>\n",
       "      <td>0.204006</td>\n",
       "      <td>0.177906</td>\n",
       "      <td>0.183219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.266617</td>\n",
       "      <td>0.314985</td>\n",
       "      <td>0.205306</td>\n",
       "      <td>0.196472</td>\n",
       "      <td>0.199323</td>\n",
       "      <td>0.219395</td>\n",
       "      <td>0.238793</td>\n",
       "      <td>0.290982</td>\n",
       "      <td>0.253710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.238921</td>\n",
       "      <td>0.332329</td>\n",
       "      <td>0.346275</td>\n",
       "      <td>0.352805</td>\n",
       "      <td>0.597053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702889</td>\n",
       "      <td>0.471683</td>\n",
       "      <td>0.327578</td>\n",
       "      <td>0.288621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171767</td>\n",
       "      <td>0.252288</td>\n",
       "      <td>0.162404</td>\n",
       "      <td>0.164073</td>\n",
       "      <td>0.133464</td>\n",
       "      <td>0.166758</td>\n",
       "      <td>0.161333</td>\n",
       "      <td>0.203986</td>\n",
       "      <td>0.220573</td>\n",
       "      <td>0.178158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.260815</td>\n",
       "      <td>0.279040</td>\n",
       "      <td>0.190434</td>\n",
       "      <td>0.246440</td>\n",
       "      <td>0.335422</td>\n",
       "      <td>0.702889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675774</td>\n",
       "      <td>0.470580</td>\n",
       "      <td>0.425448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184152</td>\n",
       "      <td>0.144051</td>\n",
       "      <td>0.046403</td>\n",
       "      <td>0.163074</td>\n",
       "      <td>0.195541</td>\n",
       "      <td>0.174143</td>\n",
       "      <td>0.186324</td>\n",
       "      <td>0.242646</td>\n",
       "      <td>0.183578</td>\n",
       "      <td>0.222493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.355523</td>\n",
       "      <td>0.334615</td>\n",
       "      <td>0.237884</td>\n",
       "      <td>0.246742</td>\n",
       "      <td>0.204006</td>\n",
       "      <td>0.471683</td>\n",
       "      <td>0.675774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778577</td>\n",
       "      <td>0.652525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260692</td>\n",
       "      <td>0.219038</td>\n",
       "      <td>0.102447</td>\n",
       "      <td>0.234008</td>\n",
       "      <td>0.239551</td>\n",
       "      <td>0.276819</td>\n",
       "      <td>0.267212</td>\n",
       "      <td>0.287603</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.146216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.353420</td>\n",
       "      <td>0.316733</td>\n",
       "      <td>0.252691</td>\n",
       "      <td>0.247078</td>\n",
       "      <td>0.177906</td>\n",
       "      <td>0.327578</td>\n",
       "      <td>0.470580</td>\n",
       "      <td>0.778577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174873</td>\n",
       "      <td>0.207996</td>\n",
       "      <td>0.105352</td>\n",
       "      <td>0.202615</td>\n",
       "      <td>0.179342</td>\n",
       "      <td>0.232764</td>\n",
       "      <td>0.193963</td>\n",
       "      <td>0.231745</td>\n",
       "      <td>0.097293</td>\n",
       "      <td>0.095243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.318276</td>\n",
       "      <td>0.270782</td>\n",
       "      <td>0.219637</td>\n",
       "      <td>0.237769</td>\n",
       "      <td>0.183219</td>\n",
       "      <td>0.288621</td>\n",
       "      <td>0.425448</td>\n",
       "      <td>0.652525</td>\n",
       "      <td>0.877131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167096</td>\n",
       "      <td>0.165537</td>\n",
       "      <td>0.097544</td>\n",
       "      <td>0.146725</td>\n",
       "      <td>0.175254</td>\n",
       "      <td>0.151889</td>\n",
       "      <td>0.140327</td>\n",
       "      <td>0.212277</td>\n",
       "      <td>0.058273</td>\n",
       "      <td>0.097358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.344058</td>\n",
       "      <td>0.297065</td>\n",
       "      <td>0.274610</td>\n",
       "      <td>0.271881</td>\n",
       "      <td>0.231684</td>\n",
       "      <td>0.333570</td>\n",
       "      <td>0.396588</td>\n",
       "      <td>0.584583</td>\n",
       "      <td>0.728063</td>\n",
       "      <td>0.853140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157615</td>\n",
       "      <td>0.165748</td>\n",
       "      <td>0.084801</td>\n",
       "      <td>0.142572</td>\n",
       "      <td>0.228991</td>\n",
       "      <td>0.122332</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>0.193358</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.089695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.210861</td>\n",
       "      <td>0.194102</td>\n",
       "      <td>0.214807</td>\n",
       "      <td>0.175381</td>\n",
       "      <td>0.211657</td>\n",
       "      <td>0.344451</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.328329</td>\n",
       "      <td>0.363404</td>\n",
       "      <td>0.485392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113418</td>\n",
       "      <td>0.117699</td>\n",
       "      <td>0.042263</td>\n",
       "      <td>0.078457</td>\n",
       "      <td>0.164590</td>\n",
       "      <td>0.115658</td>\n",
       "      <td>0.030732</td>\n",
       "      <td>0.065273</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>0.071364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.210722</td>\n",
       "      <td>0.249596</td>\n",
       "      <td>0.258767</td>\n",
       "      <td>0.215754</td>\n",
       "      <td>0.299086</td>\n",
       "      <td>0.411107</td>\n",
       "      <td>0.365391</td>\n",
       "      <td>0.322951</td>\n",
       "      <td>0.316899</td>\n",
       "      <td>0.405370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203347</td>\n",
       "      <td>0.147479</td>\n",
       "      <td>0.058599</td>\n",
       "      <td>0.160916</td>\n",
       "      <td>0.272492</td>\n",
       "      <td>0.183743</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>0.171140</td>\n",
       "      <td>0.151804</td>\n",
       "      <td>0.061411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.256278</td>\n",
       "      <td>0.273170</td>\n",
       "      <td>0.291724</td>\n",
       "      <td>0.286708</td>\n",
       "      <td>0.359062</td>\n",
       "      <td>0.396233</td>\n",
       "      <td>0.409576</td>\n",
       "      <td>0.387114</td>\n",
       "      <td>0.329659</td>\n",
       "      <td>0.345684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180464</td>\n",
       "      <td>0.137443</td>\n",
       "      <td>0.133196</td>\n",
       "      <td>0.210925</td>\n",
       "      <td>0.326821</td>\n",
       "      <td>0.252166</td>\n",
       "      <td>0.190886</td>\n",
       "      <td>0.258675</td>\n",
       "      <td>0.209122</td>\n",
       "      <td>0.120966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.304878</td>\n",
       "      <td>0.307599</td>\n",
       "      <td>0.285663</td>\n",
       "      <td>0.278529</td>\n",
       "      <td>0.318059</td>\n",
       "      <td>0.367908</td>\n",
       "      <td>0.411692</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.299575</td>\n",
       "      <td>0.294699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153162</td>\n",
       "      <td>0.135271</td>\n",
       "      <td>0.103444</td>\n",
       "      <td>0.218703</td>\n",
       "      <td>0.261822</td>\n",
       "      <td>0.218395</td>\n",
       "      <td>0.202511</td>\n",
       "      <td>0.225545</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0.171089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.239079</td>\n",
       "      <td>0.261844</td>\n",
       "      <td>0.237017</td>\n",
       "      <td>0.248245</td>\n",
       "      <td>0.328725</td>\n",
       "      <td>0.353783</td>\n",
       "      <td>0.363086</td>\n",
       "      <td>0.322237</td>\n",
       "      <td>0.241819</td>\n",
       "      <td>0.242869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099892</td>\n",
       "      <td>0.104039</td>\n",
       "      <td>0.096325</td>\n",
       "      <td>0.206922</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.215478</td>\n",
       "      <td>0.191736</td>\n",
       "      <td>0.198019</td>\n",
       "      <td>0.182337</td>\n",
       "      <td>0.158438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.137845</td>\n",
       "      <td>0.152170</td>\n",
       "      <td>0.201093</td>\n",
       "      <td>0.223203</td>\n",
       "      <td>0.326477</td>\n",
       "      <td>0.293190</td>\n",
       "      <td>0.250024</td>\n",
       "      <td>0.140912</td>\n",
       "      <td>0.100146</td>\n",
       "      <td>0.121264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>0.020313</td>\n",
       "      <td>0.035635</td>\n",
       "      <td>0.129138</td>\n",
       "      <td>0.168460</td>\n",
       "      <td>0.128968</td>\n",
       "      <td>0.145708</td>\n",
       "      <td>0.148563</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.093992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.042870</td>\n",
       "      <td>0.120587</td>\n",
       "      <td>0.194992</td>\n",
       "      <td>0.299266</td>\n",
       "      <td>0.235778</td>\n",
       "      <td>0.208057</td>\n",
       "      <td>0.061333</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.063745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104656</td>\n",
       "      <td>-0.057236</td>\n",
       "      <td>-0.006627</td>\n",
       "      <td>0.072344</td>\n",
       "      <td>0.093767</td>\n",
       "      <td>0.080812</td>\n",
       "      <td>0.056930</td>\n",
       "      <td>0.096022</td>\n",
       "      <td>0.028446</td>\n",
       "      <td>0.046617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.055227</td>\n",
       "      <td>0.040911</td>\n",
       "      <td>0.099303</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.340543</td>\n",
       "      <td>0.226305</td>\n",
       "      <td>0.215495</td>\n",
       "      <td>0.061825</td>\n",
       "      <td>0.067237</td>\n",
       "      <td>0.099632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050988</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.051367</td>\n",
       "      <td>0.120153</td>\n",
       "      <td>0.099082</td>\n",
       "      <td>0.121331</td>\n",
       "      <td>0.045204</td>\n",
       "      <td>0.138365</td>\n",
       "      <td>0.023019</td>\n",
       "      <td>0.007468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.156760</td>\n",
       "      <td>0.102428</td>\n",
       "      <td>0.103117</td>\n",
       "      <td>0.188317</td>\n",
       "      <td>0.285737</td>\n",
       "      <td>0.206841</td>\n",
       "      <td>0.196496</td>\n",
       "      <td>0.204950</td>\n",
       "      <td>0.266455</td>\n",
       "      <td>0.246924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022960</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.069692</td>\n",
       "      <td>0.171936</td>\n",
       "      <td>0.157272</td>\n",
       "      <td>0.178498</td>\n",
       "      <td>0.066425</td>\n",
       "      <td>0.132453</td>\n",
       "      <td>-0.005364</td>\n",
       "      <td>-0.028540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.117663</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.063990</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.205088</td>\n",
       "      <td>0.174768</td>\n",
       "      <td>0.165827</td>\n",
       "      <td>0.208785</td>\n",
       "      <td>0.264109</td>\n",
       "      <td>0.240862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024222</td>\n",
       "      <td>-0.034845</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.167327</td>\n",
       "      <td>0.059823</td>\n",
       "      <td>0.139089</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.079818</td>\n",
       "      <td>-0.049413</td>\n",
       "      <td>-0.025201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.056973</td>\n",
       "      <td>-0.074157</td>\n",
       "      <td>-0.026815</td>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.152897</td>\n",
       "      <td>0.123770</td>\n",
       "      <td>0.063773</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.070381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061054</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>-0.094203</td>\n",
       "      <td>0.045224</td>\n",
       "      <td>-0.119720</td>\n",
       "      <td>-0.030877</td>\n",
       "      <td>-0.069909</td>\n",
       "      <td>-0.035829</td>\n",
       "      <td>-0.143209</td>\n",
       "      <td>-0.085696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.163426</td>\n",
       "      <td>-0.179365</td>\n",
       "      <td>-0.073400</td>\n",
       "      <td>-0.029749</td>\n",
       "      <td>0.073934</td>\n",
       "      <td>0.064081</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>-0.092087</td>\n",
       "      <td>-0.154752</td>\n",
       "      <td>-0.094887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108172</td>\n",
       "      <td>-0.108934</td>\n",
       "      <td>-0.152691</td>\n",
       "      <td>-0.055641</td>\n",
       "      <td>-0.198577</td>\n",
       "      <td>-0.138900</td>\n",
       "      <td>-0.098291</td>\n",
       "      <td>-0.105235</td>\n",
       "      <td>-0.168149</td>\n",
       "      <td>-0.163696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.218093</td>\n",
       "      <td>-0.196469</td>\n",
       "      <td>-0.085380</td>\n",
       "      <td>-0.102975</td>\n",
       "      <td>-0.000624</td>\n",
       "      <td>0.027026</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>-0.124427</td>\n",
       "      <td>-0.189343</td>\n",
       "      <td>-0.178304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143650</td>\n",
       "      <td>-0.175022</td>\n",
       "      <td>-0.225897</td>\n",
       "      <td>-0.125419</td>\n",
       "      <td>-0.229297</td>\n",
       "      <td>-0.178320</td>\n",
       "      <td>-0.117429</td>\n",
       "      <td>-0.210556</td>\n",
       "      <td>-0.186527</td>\n",
       "      <td>-0.190877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.295683</td>\n",
       "      <td>-0.295302</td>\n",
       "      <td>-0.214256</td>\n",
       "      <td>-0.206673</td>\n",
       "      <td>-0.067296</td>\n",
       "      <td>-0.043280</td>\n",
       "      <td>-0.057147</td>\n",
       "      <td>-0.196354</td>\n",
       "      <td>-0.198658</td>\n",
       "      <td>-0.179890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200752</td>\n",
       "      <td>-0.250479</td>\n",
       "      <td>-0.238478</td>\n",
       "      <td>-0.219817</td>\n",
       "      <td>-0.276419</td>\n",
       "      <td>-0.187789</td>\n",
       "      <td>-0.157967</td>\n",
       "      <td>-0.270222</td>\n",
       "      <td>-0.303155</td>\n",
       "      <td>-0.253233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.342865</td>\n",
       "      <td>-0.365749</td>\n",
       "      <td>-0.291974</td>\n",
       "      <td>-0.291357</td>\n",
       "      <td>-0.125675</td>\n",
       "      <td>-0.100309</td>\n",
       "      <td>-0.126074</td>\n",
       "      <td>-0.203178</td>\n",
       "      <td>-0.137459</td>\n",
       "      <td>-0.109051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191554</td>\n",
       "      <td>-0.256166</td>\n",
       "      <td>-0.248400</td>\n",
       "      <td>-0.277169</td>\n",
       "      <td>-0.353657</td>\n",
       "      <td>-0.215894</td>\n",
       "      <td>-0.254240</td>\n",
       "      <td>-0.303427</td>\n",
       "      <td>-0.385725</td>\n",
       "      <td>-0.303949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.341703</td>\n",
       "      <td>-0.337046</td>\n",
       "      <td>-0.263111</td>\n",
       "      <td>-0.294749</td>\n",
       "      <td>-0.169618</td>\n",
       "      <td>-0.129094</td>\n",
       "      <td>-0.179526</td>\n",
       "      <td>-0.233332</td>\n",
       "      <td>-0.119143</td>\n",
       "      <td>-0.095820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137886</td>\n",
       "      <td>-0.191707</td>\n",
       "      <td>-0.259825</td>\n",
       "      <td>-0.269558</td>\n",
       "      <td>-0.347931</td>\n",
       "      <td>-0.263403</td>\n",
       "      <td>-0.267069</td>\n",
       "      <td>-0.321868</td>\n",
       "      <td>-0.360340</td>\n",
       "      <td>-0.267596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.224340</td>\n",
       "      <td>-0.234386</td>\n",
       "      <td>-0.256674</td>\n",
       "      <td>-0.256074</td>\n",
       "      <td>-0.214692</td>\n",
       "      <td>-0.118645</td>\n",
       "      <td>-0.116848</td>\n",
       "      <td>-0.120343</td>\n",
       "      <td>-0.028002</td>\n",
       "      <td>-0.052303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027750</td>\n",
       "      <td>-0.064730</td>\n",
       "      <td>-0.148791</td>\n",
       "      <td>-0.213300</td>\n",
       "      <td>-0.262620</td>\n",
       "      <td>-0.198093</td>\n",
       "      <td>-0.190854</td>\n",
       "      <td>-0.261443</td>\n",
       "      <td>-0.275442</td>\n",
       "      <td>-0.195130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.199099</td>\n",
       "      <td>-0.228490</td>\n",
       "      <td>-0.290728</td>\n",
       "      <td>-0.300476</td>\n",
       "      <td>-0.283863</td>\n",
       "      <td>-0.156081</td>\n",
       "      <td>-0.129694</td>\n",
       "      <td>-0.139750</td>\n",
       "      <td>-0.093413</td>\n",
       "      <td>-0.137173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.054779</td>\n",
       "      <td>-0.130527</td>\n",
       "      <td>-0.235110</td>\n",
       "      <td>-0.246181</td>\n",
       "      <td>-0.221273</td>\n",
       "      <td>-0.228155</td>\n",
       "      <td>-0.267938</td>\n",
       "      <td>-0.247318</td>\n",
       "      <td>-0.203776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.077430</td>\n",
       "      <td>-0.115301</td>\n",
       "      <td>-0.197493</td>\n",
       "      <td>-0.236602</td>\n",
       "      <td>-0.273350</td>\n",
       "      <td>-0.151186</td>\n",
       "      <td>-0.068142</td>\n",
       "      <td>-0.017654</td>\n",
       "      <td>0.053398</td>\n",
       "      <td>-0.043998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038928</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>-0.034937</td>\n",
       "      <td>-0.149564</td>\n",
       "      <td>-0.127523</td>\n",
       "      <td>-0.063403</td>\n",
       "      <td>-0.072976</td>\n",
       "      <td>-0.134302</td>\n",
       "      <td>-0.129402</td>\n",
       "      <td>-0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.048370</td>\n",
       "      <td>-0.055862</td>\n",
       "      <td>-0.106198</td>\n",
       "      <td>-0.190086</td>\n",
       "      <td>-0.214336</td>\n",
       "      <td>-0.054136</td>\n",
       "      <td>-0.096945</td>\n",
       "      <td>-0.081072</td>\n",
       "      <td>-0.041649</td>\n",
       "      <td>-0.091193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048936</td>\n",
       "      <td>0.087360</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>-0.146485</td>\n",
       "      <td>-0.080546</td>\n",
       "      <td>-0.067373</td>\n",
       "      <td>-0.018733</td>\n",
       "      <td>-0.036092</td>\n",
       "      <td>-0.044197</td>\n",
       "      <td>-0.043015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.030444</td>\n",
       "      <td>-0.049683</td>\n",
       "      <td>-0.109895</td>\n",
       "      <td>-0.169987</td>\n",
       "      <td>-0.173485</td>\n",
       "      <td>-0.051934</td>\n",
       "      <td>-0.115871</td>\n",
       "      <td>-0.108115</td>\n",
       "      <td>-0.028629</td>\n",
       "      <td>-0.058493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059594</td>\n",
       "      <td>0.090863</td>\n",
       "      <td>0.017997</td>\n",
       "      <td>-0.089302</td>\n",
       "      <td>-0.012792</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>0.010611</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>0.013499</td>\n",
       "      <td>-0.023863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.031939</td>\n",
       "      <td>-0.108272</td>\n",
       "      <td>-0.170671</td>\n",
       "      <td>-0.164651</td>\n",
       "      <td>-0.200586</td>\n",
       "      <td>-0.144391</td>\n",
       "      <td>-0.127052</td>\n",
       "      <td>-0.087246</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.027245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.029192</td>\n",
       "      <td>-0.037753</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.030027</td>\n",
       "      <td>0.045806</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.054285</td>\n",
       "      <td>-0.015804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.031319</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>-0.099409</td>\n",
       "      <td>-0.083965</td>\n",
       "      <td>-0.140559</td>\n",
       "      <td>-0.070337</td>\n",
       "      <td>-0.077662</td>\n",
       "      <td>-0.014578</td>\n",
       "      <td>0.013594</td>\n",
       "      <td>-0.021291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.008364</td>\n",
       "      <td>0.095110</td>\n",
       "      <td>0.064450</td>\n",
       "      <td>0.089024</td>\n",
       "      <td>0.109288</td>\n",
       "      <td>0.106959</td>\n",
       "      <td>0.083192</td>\n",
       "      <td>0.138214</td>\n",
       "      <td>0.075686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.098118</td>\n",
       "      <td>0.115824</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>-0.086529</td>\n",
       "      <td>-0.028815</td>\n",
       "      <td>-0.015531</td>\n",
       "      <td>0.035733</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>-0.035765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.052650</td>\n",
       "      <td>0.122798</td>\n",
       "      <td>0.138357</td>\n",
       "      <td>0.110776</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.168361</td>\n",
       "      <td>0.143897</td>\n",
       "      <td>0.227783</td>\n",
       "      <td>0.191193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.080722</td>\n",
       "      <td>0.132611</td>\n",
       "      <td>0.053070</td>\n",
       "      <td>0.039282</td>\n",
       "      <td>-0.073481</td>\n",
       "      <td>-0.023621</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.087187</td>\n",
       "      <td>0.036120</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.072182</td>\n",
       "      <td>0.136711</td>\n",
       "      <td>0.074314</td>\n",
       "      <td>0.069959</td>\n",
       "      <td>0.189471</td>\n",
       "      <td>0.106275</td>\n",
       "      <td>0.222683</td>\n",
       "      <td>0.176982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.169186</td>\n",
       "      <td>0.107530</td>\n",
       "      <td>0.063486</td>\n",
       "      <td>-0.064617</td>\n",
       "      <td>-0.064798</td>\n",
       "      <td>-0.001376</td>\n",
       "      <td>0.110739</td>\n",
       "      <td>0.111769</td>\n",
       "      <td>0.085072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028291</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.079799</td>\n",
       "      <td>0.130427</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.116549</td>\n",
       "      <td>0.180789</td>\n",
       "      <td>0.110760</td>\n",
       "      <td>0.163162</td>\n",
       "      <td>0.166263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.209873</td>\n",
       "      <td>0.217494</td>\n",
       "      <td>0.130276</td>\n",
       "      <td>0.089887</td>\n",
       "      <td>-0.008620</td>\n",
       "      <td>-0.048745</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.186609</td>\n",
       "      <td>0.223983</td>\n",
       "      <td>0.175717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094205</td>\n",
       "      <td>0.134015</td>\n",
       "      <td>0.171104</td>\n",
       "      <td>0.206931</td>\n",
       "      <td>0.235457</td>\n",
       "      <td>0.217587</td>\n",
       "      <td>0.156320</td>\n",
       "      <td>0.169710</td>\n",
       "      <td>0.206001</td>\n",
       "      <td>0.233288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.208371</td>\n",
       "      <td>0.186828</td>\n",
       "      <td>0.110499</td>\n",
       "      <td>0.089346</td>\n",
       "      <td>0.063408</td>\n",
       "      <td>0.030599</td>\n",
       "      <td>0.080942</td>\n",
       "      <td>0.206145</td>\n",
       "      <td>0.211897</td>\n",
       "      <td>0.233833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124038</td>\n",
       "      <td>0.108564</td>\n",
       "      <td>0.167599</td>\n",
       "      <td>0.200116</td>\n",
       "      <td>0.294578</td>\n",
       "      <td>0.223133</td>\n",
       "      <td>0.143131</td>\n",
       "      <td>0.218912</td>\n",
       "      <td>0.231150</td>\n",
       "      <td>0.222611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.099993</td>\n",
       "      <td>0.098350</td>\n",
       "      <td>0.074137</td>\n",
       "      <td>0.045141</td>\n",
       "      <td>0.061616</td>\n",
       "      <td>0.081119</td>\n",
       "      <td>0.112673</td>\n",
       "      <td>0.184411</td>\n",
       "      <td>0.122735</td>\n",
       "      <td>0.177357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>0.128310</td>\n",
       "      <td>0.121381</td>\n",
       "      <td>0.157435</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.105603</td>\n",
       "      <td>0.143718</td>\n",
       "      <td>0.189058</td>\n",
       "      <td>0.202034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.127313</td>\n",
       "      <td>0.188226</td>\n",
       "      <td>0.189047</td>\n",
       "      <td>0.145241</td>\n",
       "      <td>0.098832</td>\n",
       "      <td>0.075797</td>\n",
       "      <td>0.041071</td>\n",
       "      <td>0.097517</td>\n",
       "      <td>0.019589</td>\n",
       "      <td>-0.002523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277471</td>\n",
       "      <td>0.255774</td>\n",
       "      <td>0.254064</td>\n",
       "      <td>0.181579</td>\n",
       "      <td>0.177851</td>\n",
       "      <td>0.220670</td>\n",
       "      <td>0.193532</td>\n",
       "      <td>0.196282</td>\n",
       "      <td>0.304521</td>\n",
       "      <td>0.281889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.261345</td>\n",
       "      <td>0.233442</td>\n",
       "      <td>0.144693</td>\n",
       "      <td>0.125181</td>\n",
       "      <td>0.048763</td>\n",
       "      <td>-0.028720</td>\n",
       "      <td>0.076054</td>\n",
       "      <td>-0.005785</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428751</td>\n",
       "      <td>0.359439</td>\n",
       "      <td>0.283622</td>\n",
       "      <td>0.214580</td>\n",
       "      <td>0.175505</td>\n",
       "      <td>0.157192</td>\n",
       "      <td>0.157646</td>\n",
       "      <td>0.201077</td>\n",
       "      <td>0.276762</td>\n",
       "      <td>0.220597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.206057</td>\n",
       "      <td>0.186368</td>\n",
       "      <td>0.113920</td>\n",
       "      <td>0.050629</td>\n",
       "      <td>0.063706</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>-0.025727</td>\n",
       "      <td>0.114721</td>\n",
       "      <td>0.052409</td>\n",
       "      <td>0.076138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397190</td>\n",
       "      <td>0.302861</td>\n",
       "      <td>0.253203</td>\n",
       "      <td>0.155339</td>\n",
       "      <td>0.097671</td>\n",
       "      <td>0.123574</td>\n",
       "      <td>0.104120</td>\n",
       "      <td>0.210814</td>\n",
       "      <td>0.199334</td>\n",
       "      <td>0.161416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.157949</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>0.071946</td>\n",
       "      <td>-0.008407</td>\n",
       "      <td>0.031575</td>\n",
       "      <td>0.048870</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.135426</td>\n",
       "      <td>0.215710</td>\n",
       "      <td>0.216742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316501</td>\n",
       "      <td>0.217849</td>\n",
       "      <td>0.139544</td>\n",
       "      <td>0.095210</td>\n",
       "      <td>0.097255</td>\n",
       "      <td>0.133169</td>\n",
       "      <td>0.108185</td>\n",
       "      <td>0.109166</td>\n",
       "      <td>0.154547</td>\n",
       "      <td>0.108190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.279968</td>\n",
       "      <td>0.285716</td>\n",
       "      <td>0.180734</td>\n",
       "      <td>0.087824</td>\n",
       "      <td>0.089202</td>\n",
       "      <td>0.085468</td>\n",
       "      <td>0.110813</td>\n",
       "      <td>0.240176</td>\n",
       "      <td>0.320573</td>\n",
       "      <td>0.287459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416973</td>\n",
       "      <td>0.350208</td>\n",
       "      <td>0.181292</td>\n",
       "      <td>0.162879</td>\n",
       "      <td>0.242757</td>\n",
       "      <td>0.170750</td>\n",
       "      <td>0.144281</td>\n",
       "      <td>0.167337</td>\n",
       "      <td>0.178402</td>\n",
       "      <td>0.157181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.319354</td>\n",
       "      <td>0.304247</td>\n",
       "      <td>0.173649</td>\n",
       "      <td>0.080012</td>\n",
       "      <td>0.081964</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>0.076537</td>\n",
       "      <td>0.169099</td>\n",
       "      <td>0.195447</td>\n",
       "      <td>0.138447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505304</td>\n",
       "      <td>0.429309</td>\n",
       "      <td>0.236971</td>\n",
       "      <td>0.187964</td>\n",
       "      <td>0.269119</td>\n",
       "      <td>0.178182</td>\n",
       "      <td>0.162125</td>\n",
       "      <td>0.237890</td>\n",
       "      <td>0.205291</td>\n",
       "      <td>0.180691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.230343</td>\n",
       "      <td>0.255797</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.046109</td>\n",
       "      <td>0.041419</td>\n",
       "      <td>0.016640</td>\n",
       "      <td>0.098925</td>\n",
       "      <td>0.109744</td>\n",
       "      <td>0.084191</td>\n",
       "      <td>0.090662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570575</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.206970</td>\n",
       "      <td>0.159920</td>\n",
       "      <td>0.194223</td>\n",
       "      <td>0.146042</td>\n",
       "      <td>0.157815</td>\n",
       "      <td>0.240471</td>\n",
       "      <td>0.209045</td>\n",
       "      <td>0.139727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.203234</td>\n",
       "      <td>0.265279</td>\n",
       "      <td>0.234896</td>\n",
       "      <td>0.121065</td>\n",
       "      <td>0.084435</td>\n",
       "      <td>0.067196</td>\n",
       "      <td>0.155221</td>\n",
       "      <td>0.222783</td>\n",
       "      <td>0.225667</td>\n",
       "      <td>0.268123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573572</td>\n",
       "      <td>0.365149</td>\n",
       "      <td>0.206376</td>\n",
       "      <td>0.209084</td>\n",
       "      <td>0.210950</td>\n",
       "      <td>0.219052</td>\n",
       "      <td>0.196814</td>\n",
       "      <td>0.270198</td>\n",
       "      <td>0.221425</td>\n",
       "      <td>0.123666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.247560</td>\n",
       "      <td>0.313995</td>\n",
       "      <td>0.223074</td>\n",
       "      <td>0.133294</td>\n",
       "      <td>0.088128</td>\n",
       "      <td>0.080729</td>\n",
       "      <td>0.194720</td>\n",
       "      <td>0.271422</td>\n",
       "      <td>0.222135</td>\n",
       "      <td>0.264885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526095</td>\n",
       "      <td>0.319286</td>\n",
       "      <td>0.150871</td>\n",
       "      <td>0.195826</td>\n",
       "      <td>0.230033</td>\n",
       "      <td>0.155186</td>\n",
       "      <td>0.173098</td>\n",
       "      <td>0.328238</td>\n",
       "      <td>0.209152</td>\n",
       "      <td>0.088640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.269287</td>\n",
       "      <td>0.245868</td>\n",
       "      <td>0.081096</td>\n",
       "      <td>0.077925</td>\n",
       "      <td>0.066751</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.166112</td>\n",
       "      <td>0.191615</td>\n",
       "      <td>0.150527</td>\n",
       "      <td>0.162010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447926</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.279681</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>0.287612</td>\n",
       "      <td>0.235053</td>\n",
       "      <td>0.201609</td>\n",
       "      <td>0.342866</td>\n",
       "      <td>0.178118</td>\n",
       "      <td>0.139944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.254450</td>\n",
       "      <td>0.320538</td>\n",
       "      <td>0.238110</td>\n",
       "      <td>0.174676</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.171767</td>\n",
       "      <td>0.184152</td>\n",
       "      <td>0.260692</td>\n",
       "      <td>0.174873</td>\n",
       "      <td>0.167096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.627038</td>\n",
       "      <td>0.330396</td>\n",
       "      <td>0.384052</td>\n",
       "      <td>0.278935</td>\n",
       "      <td>0.209752</td>\n",
       "      <td>0.191407</td>\n",
       "      <td>0.325665</td>\n",
       "      <td>0.317942</td>\n",
       "      <td>0.246764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.355299</td>\n",
       "      <td>0.434548</td>\n",
       "      <td>0.394076</td>\n",
       "      <td>0.374651</td>\n",
       "      <td>0.266617</td>\n",
       "      <td>0.252288</td>\n",
       "      <td>0.144051</td>\n",
       "      <td>0.219038</td>\n",
       "      <td>0.207996</td>\n",
       "      <td>0.165537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540414</td>\n",
       "      <td>0.343190</td>\n",
       "      <td>0.337581</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.191264</td>\n",
       "      <td>0.309673</td>\n",
       "      <td>0.298711</td>\n",
       "      <td>0.195379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.311729</td>\n",
       "      <td>0.346076</td>\n",
       "      <td>0.332914</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.314985</td>\n",
       "      <td>0.162404</td>\n",
       "      <td>0.046403</td>\n",
       "      <td>0.102447</td>\n",
       "      <td>0.105352</td>\n",
       "      <td>0.097544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330396</td>\n",
       "      <td>0.540414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.412337</td>\n",
       "      <td>0.315656</td>\n",
       "      <td>0.421588</td>\n",
       "      <td>0.308197</td>\n",
       "      <td>0.370764</td>\n",
       "      <td>0.346095</td>\n",
       "      <td>0.280780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.322299</td>\n",
       "      <td>0.383960</td>\n",
       "      <td>0.367186</td>\n",
       "      <td>0.334211</td>\n",
       "      <td>0.205306</td>\n",
       "      <td>0.164073</td>\n",
       "      <td>0.163074</td>\n",
       "      <td>0.234008</td>\n",
       "      <td>0.202615</td>\n",
       "      <td>0.146725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384052</td>\n",
       "      <td>0.343190</td>\n",
       "      <td>0.412337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455059</td>\n",
       "      <td>0.397378</td>\n",
       "      <td>0.361443</td>\n",
       "      <td>0.404117</td>\n",
       "      <td>0.447118</td>\n",
       "      <td>0.283471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.312067</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>0.289731</td>\n",
       "      <td>0.284955</td>\n",
       "      <td>0.196472</td>\n",
       "      <td>0.133464</td>\n",
       "      <td>0.195541</td>\n",
       "      <td>0.239551</td>\n",
       "      <td>0.179342</td>\n",
       "      <td>0.175254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278935</td>\n",
       "      <td>0.337581</td>\n",
       "      <td>0.315656</td>\n",
       "      <td>0.455059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.429948</td>\n",
       "      <td>0.387204</td>\n",
       "      <td>0.503465</td>\n",
       "      <td>0.453658</td>\n",
       "      <td>0.264399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.220642</td>\n",
       "      <td>0.262263</td>\n",
       "      <td>0.287661</td>\n",
       "      <td>0.280938</td>\n",
       "      <td>0.199323</td>\n",
       "      <td>0.166758</td>\n",
       "      <td>0.174143</td>\n",
       "      <td>0.276819</td>\n",
       "      <td>0.232764</td>\n",
       "      <td>0.151889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209752</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.421588</td>\n",
       "      <td>0.397378</td>\n",
       "      <td>0.429948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515154</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>0.430804</td>\n",
       "      <td>0.349449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.280341</td>\n",
       "      <td>0.380819</td>\n",
       "      <td>0.340254</td>\n",
       "      <td>0.219395</td>\n",
       "      <td>0.161333</td>\n",
       "      <td>0.186324</td>\n",
       "      <td>0.267212</td>\n",
       "      <td>0.193963</td>\n",
       "      <td>0.140327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191407</td>\n",
       "      <td>0.191264</td>\n",
       "      <td>0.308197</td>\n",
       "      <td>0.361443</td>\n",
       "      <td>0.387204</td>\n",
       "      <td>0.515154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509805</td>\n",
       "      <td>0.431295</td>\n",
       "      <td>0.287219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.368132</td>\n",
       "      <td>0.353042</td>\n",
       "      <td>0.334108</td>\n",
       "      <td>0.344865</td>\n",
       "      <td>0.238793</td>\n",
       "      <td>0.203986</td>\n",
       "      <td>0.242646</td>\n",
       "      <td>0.287603</td>\n",
       "      <td>0.231745</td>\n",
       "      <td>0.212277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325665</td>\n",
       "      <td>0.309673</td>\n",
       "      <td>0.370764</td>\n",
       "      <td>0.404117</td>\n",
       "      <td>0.503465</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>0.509805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550235</td>\n",
       "      <td>0.329827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.357116</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.425047</td>\n",
       "      <td>0.420266</td>\n",
       "      <td>0.290982</td>\n",
       "      <td>0.220573</td>\n",
       "      <td>0.183578</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.097293</td>\n",
       "      <td>0.058273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317942</td>\n",
       "      <td>0.298711</td>\n",
       "      <td>0.346095</td>\n",
       "      <td>0.447118</td>\n",
       "      <td>0.453658</td>\n",
       "      <td>0.430804</td>\n",
       "      <td>0.431295</td>\n",
       "      <td>0.550235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.642872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.347078</td>\n",
       "      <td>0.358761</td>\n",
       "      <td>0.373948</td>\n",
       "      <td>0.400626</td>\n",
       "      <td>0.253710</td>\n",
       "      <td>0.178158</td>\n",
       "      <td>0.222493</td>\n",
       "      <td>0.146216</td>\n",
       "      <td>0.095243</td>\n",
       "      <td>0.097358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246764</td>\n",
       "      <td>0.195379</td>\n",
       "      <td>0.280780</td>\n",
       "      <td>0.283471</td>\n",
       "      <td>0.264399</td>\n",
       "      <td>0.349449</td>\n",
       "      <td>0.287219</td>\n",
       "      <td>0.329827</td>\n",
       "      <td>0.642872</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.735896  0.571537  0.491438  0.344797  0.238921  0.260815   \n",
       "1   0.735896  1.000000  0.779916  0.606684  0.419669  0.332329  0.279040   \n",
       "2   0.571537  0.779916  1.000000  0.781786  0.546141  0.346275  0.190434   \n",
       "3   0.491438  0.606684  0.781786  1.000000  0.726943  0.352805  0.246440   \n",
       "4   0.344797  0.419669  0.546141  0.726943  1.000000  0.597053  0.335422   \n",
       "5   0.238921  0.332329  0.346275  0.352805  0.597053  1.000000  0.702889   \n",
       "6   0.260815  0.279040  0.190434  0.246440  0.335422  0.702889  1.000000   \n",
       "7   0.355523  0.334615  0.237884  0.246742  0.204006  0.471683  0.675774   \n",
       "8   0.353420  0.316733  0.252691  0.247078  0.177906  0.327578  0.470580   \n",
       "9   0.318276  0.270782  0.219637  0.237769  0.183219  0.288621  0.425448   \n",
       "10  0.344058  0.297065  0.274610  0.271881  0.231684  0.333570  0.396588   \n",
       "11  0.210861  0.194102  0.214807  0.175381  0.211657  0.344451  0.274432   \n",
       "12  0.210722  0.249596  0.258767  0.215754  0.299086  0.411107  0.365391   \n",
       "13  0.256278  0.273170  0.291724  0.286708  0.359062  0.396233  0.409576   \n",
       "14  0.304878  0.307599  0.285663  0.278529  0.318059  0.367908  0.411692   \n",
       "15  0.239079  0.261844  0.237017  0.248245  0.328725  0.353783  0.363086   \n",
       "16  0.137845  0.152170  0.201093  0.223203  0.326477  0.293190  0.250024   \n",
       "17  0.041817  0.042870  0.120587  0.194992  0.299266  0.235778  0.208057   \n",
       "18  0.055227  0.040911  0.099303  0.189405  0.340543  0.226305  0.215495   \n",
       "19  0.156760  0.102428  0.103117  0.188317  0.285737  0.206841  0.196496   \n",
       "20  0.117663  0.075255  0.063990  0.142271  0.205088  0.174768  0.165827   \n",
       "21 -0.056973 -0.074157 -0.026815  0.036010  0.152897  0.123770  0.063773   \n",
       "22 -0.163426 -0.179365 -0.073400 -0.029749  0.073934  0.064081  0.009359   \n",
       "23 -0.218093 -0.196469 -0.085380 -0.102975 -0.000624  0.027026  0.011982   \n",
       "24 -0.295683 -0.295302 -0.214256 -0.206673 -0.067296 -0.043280 -0.057147   \n",
       "25 -0.342865 -0.365749 -0.291974 -0.291357 -0.125675 -0.100309 -0.126074   \n",
       "26 -0.341703 -0.337046 -0.263111 -0.294749 -0.169618 -0.129094 -0.179526   \n",
       "27 -0.224340 -0.234386 -0.256674 -0.256074 -0.214692 -0.118645 -0.116848   \n",
       "28 -0.199099 -0.228490 -0.290728 -0.300476 -0.283863 -0.156081 -0.129694   \n",
       "29 -0.077430 -0.115301 -0.197493 -0.236602 -0.273350 -0.151186 -0.068142   \n",
       "30 -0.048370 -0.055862 -0.106198 -0.190086 -0.214336 -0.054136 -0.096945   \n",
       "31 -0.030444 -0.049683 -0.109895 -0.169987 -0.173485 -0.051934 -0.115871   \n",
       "32 -0.031939 -0.108272 -0.170671 -0.164651 -0.200586 -0.144391 -0.127052   \n",
       "33  0.031319 -0.004247 -0.099409 -0.083965 -0.140559 -0.070337 -0.077662   \n",
       "34  0.098118  0.115824  0.017053  0.015200 -0.086529 -0.028815 -0.015531   \n",
       "35  0.080722  0.132611  0.053070  0.039282 -0.073481 -0.023621  0.002979   \n",
       "36  0.119565  0.169186  0.107530  0.063486 -0.064617 -0.064798 -0.001376   \n",
       "37  0.209873  0.217494  0.130276  0.089887 -0.008620 -0.048745  0.065900   \n",
       "38  0.208371  0.186828  0.110499  0.089346  0.063408  0.030599  0.080942   \n",
       "39  0.099993  0.098350  0.074137  0.045141  0.061616  0.081119  0.112673   \n",
       "40  0.127313  0.188226  0.189047  0.145241  0.098832  0.075797  0.041071   \n",
       "41  0.213592  0.261345  0.233442  0.144693  0.125181  0.048763 -0.028720   \n",
       "42  0.206057  0.186368  0.113920  0.050629  0.063706  0.034380 -0.025727   \n",
       "43  0.157949  0.133018  0.071946 -0.008407  0.031575  0.048870  0.061404   \n",
       "44  0.279968  0.285716  0.180734  0.087824  0.089202  0.085468  0.110813   \n",
       "45  0.319354  0.304247  0.173649  0.080012  0.081964  0.029524  0.076537   \n",
       "46  0.230343  0.255797  0.179528  0.046109  0.041419  0.016640  0.098925   \n",
       "47  0.203234  0.265279  0.234896  0.121065  0.084435  0.067196  0.155221   \n",
       "48  0.247560  0.313995  0.223074  0.133294  0.088128  0.080729  0.194720   \n",
       "49  0.269287  0.245868  0.081096  0.077925  0.066751  0.017300  0.166112   \n",
       "50  0.254450  0.320538  0.238110  0.174676  0.115936  0.171767  0.184152   \n",
       "51  0.355299  0.434548  0.394076  0.374651  0.266617  0.252288  0.144051   \n",
       "52  0.311729  0.346076  0.332914  0.364772  0.314985  0.162404  0.046403   \n",
       "53  0.322299  0.383960  0.367186  0.334211  0.205306  0.164073  0.163074   \n",
       "54  0.312067  0.380165  0.289731  0.284955  0.196472  0.133464  0.195541   \n",
       "55  0.220642  0.262263  0.287661  0.280938  0.199323  0.166758  0.174143   \n",
       "56  0.313725  0.280341  0.380819  0.340254  0.219395  0.161333  0.186324   \n",
       "57  0.368132  0.353042  0.334108  0.344865  0.238793  0.203986  0.242646   \n",
       "58  0.357116  0.352200  0.425047  0.420266  0.290982  0.220573  0.183578   \n",
       "59  0.347078  0.358761  0.373948  0.400626  0.253710  0.178158  0.222493   \n",
       "\n",
       "          7         8         9   ...        50        51        52        53  \\\n",
       "0   0.355523  0.353420  0.318276  ...  0.254450  0.355299  0.311729  0.322299   \n",
       "1   0.334615  0.316733  0.270782  ...  0.320538  0.434548  0.346076  0.383960   \n",
       "2   0.237884  0.252691  0.219637  ...  0.238110  0.394076  0.332914  0.367186   \n",
       "3   0.246742  0.247078  0.237769  ...  0.174676  0.374651  0.364772  0.334211   \n",
       "4   0.204006  0.177906  0.183219  ...  0.115936  0.266617  0.314985  0.205306   \n",
       "5   0.471683  0.327578  0.288621  ...  0.171767  0.252288  0.162404  0.164073   \n",
       "6   0.675774  0.470580  0.425448  ...  0.184152  0.144051  0.046403  0.163074   \n",
       "7   1.000000  0.778577  0.652525  ...  0.260692  0.219038  0.102447  0.234008   \n",
       "8   0.778577  1.000000  0.877131  ...  0.174873  0.207996  0.105352  0.202615   \n",
       "9   0.652525  0.877131  1.000000  ...  0.167096  0.165537  0.097544  0.146725   \n",
       "10  0.584583  0.728063  0.853140  ...  0.157615  0.165748  0.084801  0.142572   \n",
       "11  0.328329  0.363404  0.485392  ...  0.113418  0.117699  0.042263  0.078457   \n",
       "12  0.322951  0.316899  0.405370  ...  0.203347  0.147479  0.058599  0.160916   \n",
       "13  0.387114  0.329659  0.345684  ...  0.180464  0.137443  0.133196  0.210925   \n",
       "14  0.391514  0.299575  0.294699  ...  0.153162  0.135271  0.103444  0.218703   \n",
       "15  0.322237  0.241819  0.242869  ...  0.099892  0.104039  0.096325  0.206922   \n",
       "16  0.140912  0.100146  0.121264  ...  0.009036  0.020313  0.035635  0.129138   \n",
       "17  0.061333  0.027380  0.063745  ... -0.104656 -0.057236 -0.006627  0.072344   \n",
       "18  0.061825  0.067237  0.099632  ... -0.050988  0.011450  0.051367  0.120153   \n",
       "19  0.204950  0.266455  0.246924  ... -0.022960  0.028754  0.069692  0.171936   \n",
       "20  0.208785  0.264109  0.240862  ... -0.024222 -0.034845 -0.000270  0.167327   \n",
       "21  0.023786  0.019512  0.070381  ... -0.061054 -0.092204 -0.094203  0.045224   \n",
       "22 -0.092087 -0.154752 -0.094887  ... -0.108172 -0.108934 -0.152691 -0.055641   \n",
       "23 -0.124427 -0.189343 -0.178304  ... -0.143650 -0.175022 -0.225897 -0.125419   \n",
       "24 -0.196354 -0.198658 -0.179890  ... -0.200752 -0.250479 -0.238478 -0.219817   \n",
       "25 -0.203178 -0.137459 -0.109051  ... -0.191554 -0.256166 -0.248400 -0.277169   \n",
       "26 -0.233332 -0.119143 -0.095820  ... -0.137886 -0.191707 -0.259825 -0.269558   \n",
       "27 -0.120343 -0.028002 -0.052303  ... -0.027750 -0.064730 -0.148791 -0.213300   \n",
       "28 -0.139750 -0.093413 -0.137173  ... -0.000251 -0.054779 -0.130527 -0.235110   \n",
       "29 -0.017654  0.053398 -0.043998  ...  0.038928  0.039053 -0.034937 -0.149564   \n",
       "30 -0.081072 -0.041649 -0.091193  ...  0.048936  0.087360  0.026300 -0.146485   \n",
       "31 -0.108115 -0.028629 -0.058493  ...  0.059594  0.090863  0.017997 -0.089302   \n",
       "32 -0.087246 -0.017885 -0.027245  ... -0.002591  0.003084  0.029192 -0.037753   \n",
       "33 -0.014578  0.013594 -0.021291  ...  0.003386  0.008364  0.095110  0.064450   \n",
       "34  0.035733  0.015065 -0.035765  ...  0.018382  0.052650  0.122798  0.138357   \n",
       "35  0.087187  0.036120 -0.004460  ...  0.006165  0.023165  0.072182  0.136711   \n",
       "36  0.110739  0.111769  0.085072  ... -0.028291  0.002078  0.079799  0.130427   \n",
       "37  0.186609  0.223983  0.175717  ...  0.094205  0.134015  0.171104  0.206931   \n",
       "38  0.206145  0.211897  0.233833  ...  0.124038  0.108564  0.167599  0.200116   \n",
       "39  0.184411  0.122735  0.177357  ...  0.066673  0.042677  0.128310  0.121381   \n",
       "40  0.097517  0.019589 -0.002523  ...  0.277471  0.255774  0.254064  0.181579   \n",
       "41  0.076054 -0.005785 -0.018880  ...  0.428751  0.359439  0.283622  0.214580   \n",
       "42  0.114721  0.052409  0.076138  ...  0.397190  0.302861  0.253203  0.155339   \n",
       "43  0.135426  0.215710  0.216742  ...  0.316501  0.217849  0.139544  0.095210   \n",
       "44  0.240176  0.320573  0.287459  ...  0.416973  0.350208  0.181292  0.162879   \n",
       "45  0.169099  0.195447  0.138447  ...  0.505304  0.429309  0.236971  0.187964   \n",
       "46  0.109744  0.084191  0.090662  ...  0.570575  0.398600  0.206970  0.159920   \n",
       "47  0.222783  0.225667  0.268123  ...  0.573572  0.365149  0.206376  0.209084   \n",
       "48  0.271422  0.222135  0.264885  ...  0.526095  0.319286  0.150871  0.195826   \n",
       "49  0.191615  0.150527  0.162010  ...  0.447926  0.341667  0.279681  0.280477   \n",
       "50  0.260692  0.174873  0.167096  ...  1.000000  0.627038  0.330396  0.384052   \n",
       "51  0.219038  0.207996  0.165537  ...  0.627038  1.000000  0.540414  0.343190   \n",
       "52  0.102447  0.105352  0.097544  ...  0.330396  0.540414  1.000000  0.412337   \n",
       "53  0.234008  0.202615  0.146725  ...  0.384052  0.343190  0.412337  1.000000   \n",
       "54  0.239551  0.179342  0.175254  ...  0.278935  0.337581  0.315656  0.455059   \n",
       "55  0.276819  0.232764  0.151889  ...  0.209752  0.203121  0.421588  0.397378   \n",
       "56  0.267212  0.193963  0.140327  ...  0.191407  0.191264  0.308197  0.361443   \n",
       "57  0.287603  0.231745  0.212277  ...  0.325665  0.309673  0.370764  0.404117   \n",
       "58  0.194400  0.097293  0.058273  ...  0.317942  0.298711  0.346095  0.447118   \n",
       "59  0.146216  0.095243  0.097358  ...  0.246764  0.195379  0.280780  0.283471   \n",
       "\n",
       "          54        55        56        57        58        59  \n",
       "0   0.312067  0.220642  0.313725  0.368132  0.357116  0.347078  \n",
       "1   0.380165  0.262263  0.280341  0.353042  0.352200  0.358761  \n",
       "2   0.289731  0.287661  0.380819  0.334108  0.425047  0.373948  \n",
       "3   0.284955  0.280938  0.340254  0.344865  0.420266  0.400626  \n",
       "4   0.196472  0.199323  0.219395  0.238793  0.290982  0.253710  \n",
       "5   0.133464  0.166758  0.161333  0.203986  0.220573  0.178158  \n",
       "6   0.195541  0.174143  0.186324  0.242646  0.183578  0.222493  \n",
       "7   0.239551  0.276819  0.267212  0.287603  0.194400  0.146216  \n",
       "8   0.179342  0.232764  0.193963  0.231745  0.097293  0.095243  \n",
       "9   0.175254  0.151889  0.140327  0.212277  0.058273  0.097358  \n",
       "10  0.228991  0.122332  0.103405  0.193358  0.067726  0.089695  \n",
       "11  0.164590  0.115658  0.030732  0.065273  0.044614  0.071364  \n",
       "12  0.272492  0.183743  0.057870  0.171140  0.151804  0.061411  \n",
       "13  0.326821  0.252166  0.190886  0.258675  0.209122  0.120966  \n",
       "14  0.261822  0.218395  0.202511  0.225545  0.193671  0.171089  \n",
       "15  0.240968  0.215478  0.191736  0.198019  0.182337  0.158438  \n",
       "16  0.168460  0.128968  0.145708  0.148563  0.121800  0.093992  \n",
       "17  0.093767  0.080812  0.056930  0.096022  0.028446  0.046617  \n",
       "18  0.099082  0.121331  0.045204  0.138365  0.023019  0.007468  \n",
       "19  0.157272  0.178498  0.066425  0.132453 -0.005364 -0.028540  \n",
       "20  0.059823  0.139089  0.030943  0.079818 -0.049413 -0.025201  \n",
       "21 -0.119720 -0.030877 -0.069909 -0.035829 -0.143209 -0.085696  \n",
       "22 -0.198577 -0.138900 -0.098291 -0.105235 -0.168149 -0.163696  \n",
       "23 -0.229297 -0.178320 -0.117429 -0.210556 -0.186527 -0.190877  \n",
       "24 -0.276419 -0.187789 -0.157967 -0.270222 -0.303155 -0.253233  \n",
       "25 -0.353657 -0.215894 -0.254240 -0.303427 -0.385725 -0.303949  \n",
       "26 -0.347931 -0.263403 -0.267069 -0.321868 -0.360340 -0.267596  \n",
       "27 -0.262620 -0.198093 -0.190854 -0.261443 -0.275442 -0.195130  \n",
       "28 -0.246181 -0.221273 -0.228155 -0.267938 -0.247318 -0.203776  \n",
       "29 -0.127523 -0.063403 -0.072976 -0.134302 -0.129402 -0.076100  \n",
       "30 -0.080546 -0.067373 -0.018733 -0.036092 -0.044197 -0.043015  \n",
       "31 -0.012792  0.017714  0.010611  0.018564  0.013499 -0.023863  \n",
       "32  0.000520  0.030027  0.045806  0.003712  0.054285 -0.015804  \n",
       "33  0.089024  0.109288  0.106959  0.083192  0.138214  0.075686  \n",
       "34  0.110776  0.131490  0.168361  0.143897  0.227783  0.191193  \n",
       "35  0.074314  0.069959  0.189471  0.106275  0.222683  0.176982  \n",
       "36  0.086914  0.116549  0.180789  0.110760  0.163162  0.166263  \n",
       "37  0.235457  0.217587  0.156320  0.169710  0.206001  0.233288  \n",
       "38  0.294578  0.223133  0.143131  0.218912  0.231150  0.222611  \n",
       "39  0.157435  0.150700  0.105603  0.143718  0.189058  0.202034  \n",
       "40  0.177851  0.220670  0.193532  0.196282  0.304521  0.281889  \n",
       "41  0.175505  0.157192  0.157646  0.201077  0.276762  0.220597  \n",
       "42  0.097671  0.123574  0.104120  0.210814  0.199334  0.161416  \n",
       "43  0.097255  0.133169  0.108185  0.109166  0.154547  0.108190  \n",
       "44  0.242757  0.170750  0.144281  0.167337  0.178402  0.157181  \n",
       "45  0.269119  0.178182  0.162125  0.237890  0.205291  0.180691  \n",
       "46  0.194223  0.146042  0.157815  0.240471  0.209045  0.139727  \n",
       "47  0.210950  0.219052  0.196814  0.270198  0.221425  0.123666  \n",
       "48  0.230033  0.155186  0.173098  0.328238  0.209152  0.088640  \n",
       "49  0.287612  0.235053  0.201609  0.342866  0.178118  0.139944  \n",
       "50  0.278935  0.209752  0.191407  0.325665  0.317942  0.246764  \n",
       "51  0.337581  0.203121  0.191264  0.309673  0.298711  0.195379  \n",
       "52  0.315656  0.421588  0.308197  0.370764  0.346095  0.280780  \n",
       "53  0.455059  0.397378  0.361443  0.404117  0.447118  0.283471  \n",
       "54  1.000000  0.429948  0.387204  0.503465  0.453658  0.264399  \n",
       "55  0.429948  1.000000  0.515154  0.463659  0.430804  0.349449  \n",
       "56  0.387204  0.515154  1.000000  0.509805  0.431295  0.287219  \n",
       "57  0.503465  0.463659  0.509805  1.000000  0.550235  0.329827  \n",
       "58  0.453658  0.430804  0.431295  0.550235  1.000000  0.642872  \n",
       "59  0.264399  0.349449  0.287219  0.329827  0.642872  1.000000  \n",
       "\n",
       "[60 rows x 60 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = sonar_data.isnull().sum()\n",
    "for i in missing_values:\n",
    "    if i>0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j912DrKe7L03"
   },
   "source": [
    "## Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bTnEFld87GIr"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "if sparse.issparse(X):\n",
    "    X = X.toarray()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify=Y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_Train, Y_Train are the training data , X_Test, Y_Test are the testing data - X_Test_predictions are the predicted values. Test data size will 0.1 times that is 10% of the original dataset while others will be training dataset. Stratify - separate based on Y - based on rock and mine, if random_state is 1 then throughout any system random state 1 will be same split of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ww4D1Ps379_h",
    "outputId": "5a7bbea6-aafb-4978-c7ef-0e28506f5ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60) (187, 60) (21, 60)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBvcm4eR8enA",
    "outputId": "4878f4d0-12c2-4d69-8f07-1b23b1b69555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2       3       4       5       6       7       8   \\\n",
      "115  0.0414  0.0436  0.0447  0.0844  0.0419  0.1215  0.2002  0.1516  0.0818   \n",
      "38   0.0123  0.0022  0.0196  0.0206  0.0180  0.0492  0.0033  0.0398  0.0791   \n",
      "56   0.0152  0.0102  0.0113  0.0263  0.0097  0.0391  0.0857  0.0915  0.0949   \n",
      "123  0.0270  0.0163  0.0341  0.0247  0.0822  0.1256  0.1323  0.1584  0.2017   \n",
      "18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "140  0.0412  0.1135  0.0518  0.0232  0.0646  0.1124  0.1787  0.2407  0.2682   \n",
      "5    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
      "154  0.0117  0.0069  0.0279  0.0583  0.0915  0.1267  0.1577  0.1927  0.2361   \n",
      "131  0.1150  0.1163  0.0866  0.0358  0.0232  0.1267  0.2417  0.2661  0.4346   \n",
      "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
      "\n",
      "         9   ...      50      51      52      53      54      55      56  \\\n",
      "115  0.1975  ...  0.0222  0.0045  0.0136  0.0113  0.0053  0.0165  0.0141   \n",
      "38   0.0475  ...  0.0149  0.0125  0.0134  0.0026  0.0038  0.0018  0.0113   \n",
      "56   0.1504  ...  0.0048  0.0049  0.0041  0.0036  0.0013  0.0046  0.0037   \n",
      "123  0.2122  ...  0.0197  0.0189  0.0204  0.0085  0.0043  0.0092  0.0138   \n",
      "18   0.1520  ...  0.0045  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120   \n",
      "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "140  0.2058  ...  0.0798  0.0376  0.0143  0.0272  0.0127  0.0166  0.0095   \n",
      "5    0.3039  ...  0.0104  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057   \n",
      "154  0.2169  ...  0.0039  0.0053  0.0029  0.0020  0.0013  0.0029  0.0020   \n",
      "131  0.5378  ...  0.0228  0.0099  0.0065  0.0085  0.0166  0.0110  0.0190   \n",
      "203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n",
      "\n",
      "         57      58      59  \n",
      "115  0.0077  0.0246  0.0198  \n",
      "38   0.0058  0.0047  0.0071  \n",
      "56   0.0011  0.0034  0.0033  \n",
      "123  0.0094  0.0105  0.0093  \n",
      "18   0.0132  0.0070  0.0088  \n",
      "..      ...     ...     ...  \n",
      "140  0.0225  0.0098  0.0085  \n",
      "5    0.0027  0.0051  0.0062  \n",
      "154  0.0062  0.0026  0.0052  \n",
      "131  0.0141  0.0068  0.0086  \n",
      "203  0.0115  0.0193  0.0157  \n",
      "\n",
      "[187 rows x 60 columns]\n",
      "115    M\n",
      "38     R\n",
      "56     R\n",
      "123    M\n",
      "18     R\n",
      "      ..\n",
      "140    M\n",
      "5      R\n",
      "154    M\n",
      "131    M\n",
      "203    M\n",
      "Name: 60, Length: 187, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKLgrLOx8LQx"
   },
   "source": [
    "## Model Training --> Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UoM3FhQS8FAw"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGomegF-8TPv",
    "outputId": "688846eb-1b29-4e7f-9905-7956d013dc72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the Logistic Regression model with training data\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "585vgP7b8vBn"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kCBykEtO8pLi"
   },
   "outputs": [],
   "source": [
    "#accuracy on training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50Wqy2Rc9nL1",
    "outputId": "cc9d8c2e-92ee-4047-e6c6-3bee7fbb359c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data :  0.8342245989304813\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on training data : ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RCUZ6MuR9tOV"
   },
   "outputs": [],
   "source": [
    "#accuracy on test data\n",
    "X_test_prediction = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04AsqCrz99vU",
    "outputId": "41e1fed1-ef90-4937-9d9a-a7d23a16504e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data :  0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on test data : ', test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.75      0.82      0.78        11\n",
      "           R       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.76        21\n",
      "weighted avg       0.76      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, X_test_prediction))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, X_test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>For Logistic Regression</strong>: We can see that the accuracy is decent - Confusion matrix shows better classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKrIzmr8-K9s"
   },
   "source": [
    "## Making a Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMp-UfOd-B7B",
    "outputId": "a7aaeda1-f37b-4719-ab0a-01571db68419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M']\n",
      "The object is a mine\n"
     ]
    }
   ],
   "source": [
    "input_data = (0.0307,0.0523,0.0653,0.0521,0.0611,0.0577,0.0665,0.0664,0.1460,0.2792,0.3877,0.4992,0.4981,0.4972,0.5607,0.7339,0.8230,0.9173,0.9975,0.9911,0.8240,0.6498,0.5980,0.4862,0.3150,0.1543,0.0989,0.0284,0.1008,0.2636,0.2694,0.2930,0.2925,0.3998,0.3660,0.3172,0.4609,0.4374,0.1820,0.3376,0.6202,0.4448,0.1863,0.1420,0.0589,0.0576,0.0672,0.0269,0.0245,0.0190,0.0063,0.0321,0.0189,0.0137,0.0277,0.0152,0.0052,0.0121,0.0124,0.0055)\n",
    "\n",
    "# changing the input_data to a numpy array\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "\n",
    "# reshape the np array as we are predicting for one instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "prediction = model.predict(input_data_reshaped)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0]=='R'):\n",
    "  print('The object is a Rock')\n",
    "else:\n",
    "  print('The object is a mine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.75      0.82      0.78        11\n",
      "           R       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.76        21\n",
      "weighted avg       0.76      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.69      0.82      0.75        11\n",
      "           R       0.75      0.60      0.67        10\n",
      "\n",
      "    accuracy                           0.71        21\n",
      "   macro avg       0.72      0.71      0.71        21\n",
      "weighted avg       0.72      0.71      0.71        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>For Logistic Regression with PCA</strong>: We can see that the accuracy is not improved but reduced than the normal Logistic regression without PCA - Confusion matrix shows worser classification for negative examples and the positive examples remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.77      0.91      0.83        11\n",
      "           R       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.82      0.80      0.81        21\n",
      "weighted avg       0.82      0.81      0.81        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  1]\n",
      " [ 3  7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "pca = PCA(n_components=10)  # You can adjust the number of components based on your needs\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>For Random forest classifier with PCA</strong>: We can see that the accuracy is improved - Confusion matrix shows better classification of positive values but the negative values remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.75      0.82      0.78        11\n",
      "           R       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.76        21\n",
      "weighted avg       0.76      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.71      0.91      0.80        11\n",
      "           R       0.86      0.60      0.71        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.79      0.75      0.75        21\n",
      "weighted avg       0.78      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  1]\n",
      " [ 4  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train the Support Vector Machine (SVM) classifier\n",
    "model = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>For Support Vector Machine with PCA</strong>: We can see that the accuracy is almost the same as logistic regression without PCA - Confusion matrix shows better classification of positive values but the negative values is only decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.73      0.73      0.73        11\n",
      "           R       0.70      0.70      0.70        10\n",
      "\n",
      "    accuracy                           0.71        21\n",
      "   macro avg       0.71      0.71      0.71        21\n",
      "weighted avg       0.71      0.71      0.71        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 3]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>For SVM without PCA</strong>: We can see that the accuracy is getting worser - The other methods have better accuracy than this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.89      0.73      0.80        11\n",
      "           R       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.82      0.81      0.81        21\n",
      "weighted avg       0.82      0.81      0.81        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 3]\n",
      " [1 9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.83      0.91      0.87        11\n",
      "           R       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.86        21\n",
      "   macro avg       0.86      0.85      0.86        21\n",
      "weighted avg       0.86      0.86      0.86        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  1]\n",
      " [ 2  8]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.83      0.91      0.87        11\n",
      "           R       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.86        21\n",
      "   macro avg       0.86      0.85      0.86        21\n",
      "weighted avg       0.86      0.86      0.86        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  1]\n",
      " [ 2  8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=1)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.82      0.82      0.82        11\n",
      "           R       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.81      0.81      0.81        21\n",
      "weighted avg       0.81      0.81      0.81        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [2 8]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.85      1.00      0.92        11\n",
      "           R       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.92      0.90      0.90        21\n",
      "weighted avg       0.92      0.90      0.90        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0]\n",
      " [ 2  8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "model.fit(X_train.values, Y_train.values)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.92      1.00      0.96        11\n",
      "           R       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0]\n",
      " [ 1  9]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Bayes without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.67      0.55      0.60        11\n",
      "           R       0.58      0.70      0.64        10\n",
      "\n",
      "    accuracy                           0.62        21\n",
      "   macro avg       0.62      0.62      0.62        21\n",
      "weighted avg       0.63      0.62      0.62        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 5]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Bayes without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.80      0.73      0.76        11\n",
      "           R       0.73      0.80      0.76        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.76        21\n",
      "weighted avg       0.77      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 3]\n",
      " [2 8]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.75      0.82      0.78        11\n",
      "           R       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.76        21\n",
      "weighted avg       0.76      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.77      0.91      0.83        11\n",
      "           R       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.82      0.80      0.81        21\n",
      "weighted avg       0.82      0.81      0.81        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  1]\n",
      " [ 3  7]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.75      0.82      0.78        11\n",
      "           R       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.76        21\n",
      "weighted avg       0.76      0.76      0.76        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [3 7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "alpha_value = 1.0\n",
    "\n",
    "# Ridge Classifier model\n",
    "model = RidgeClassifier(alpha=alpha_value)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Classifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.69      0.82      0.75        11\n",
      "           R       0.75      0.60      0.67        10\n",
      "\n",
      "    accuracy                           0.71        21\n",
      "   macro avg       0.72      0.71      0.71        21\n",
      "weighted avg       0.72      0.71      0.71        21\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 2]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Logistic Regression...\n",
      "Cross-Validation Scores: [0.78947368 0.78947368 0.89189189 0.75675676 0.72972973]\n",
      "Mean CV Score: 0.7914651493598862\n",
      "Best Parameters: {'C': 10}\n",
      "Test Set Accuracy: 0.7619047619047619\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Naive Bayes...\n",
      "Cross-Validation Scores: [0.68421053 0.65789474 0.67567568 0.78378378 0.78378378]\n",
      "Mean CV Score: 0.7170697012802275\n",
      "Best Parameters: {}\n",
      "Test Set Accuracy: 0.6190476190476191\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Decision Tree...\n",
      "Cross-Validation Scores: [0.81578947 0.76315789 0.67567568 0.64864865 0.83783784]\n",
      "Mean CV Score: 0.748221906116643\n",
      "Best Parameters: {'max_depth': None}\n",
      "Test Set Accuracy: 0.7142857142857143\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Random Forest...\n",
      "Cross-Validation Scores: [0.89473684 0.78947368 0.81081081 0.72972973 0.91891892]\n",
      "Mean CV Score: 0.8287339971550498\n",
      "Best Parameters: {'n_estimators': 200}\n",
      "Test Set Accuracy: 0.7142857142857143\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Gradient Boosting...\n",
      "Cross-Validation Scores: [0.81578947 0.73684211 0.86486486 0.81081081 0.97297297]\n",
      "Mean CV Score: 0.8402560455192034\n",
      "Best Parameters: {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Test Set Accuracy: 0.8095238095238095\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Neural Network...\n",
      "Cross-Validation Scores: [0.84210526 0.76315789 0.78378378 0.81081081 0.75675676]\n",
      "Mean CV Score: 0.7913229018492176\n",
      "Best Parameters: {'alpha': 0.001, 'hidden_layer_sizes': (50, 50)}\n",
      "Test Set Accuracy: 0.8095238095238095\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing SVM...\n",
      "Cross-Validation Scores: [0.92105263 0.63157895 0.89189189 0.72972973 0.78378378]\n",
      "Mean CV Score: 0.7916073968705548\n",
      "Best Parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Test Set Accuracy: 0.9523809523809523\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Ridge Classifier...\n",
      "Cross-Validation Scores: [0.81578947 0.78947368 0.81081081 0.75675676 0.7027027 ]\n",
      "Mean CV Score: 0.7751066856330014\n",
      "Best Parameters: {'alpha': 1}\n",
      "Test Set Accuracy: 0.7619047619047619\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from scipy import sparse\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 10, 100]}),\n",
    "    ('Naive Bayes', GaussianNB(), {}),\n",
    "    ('Decision Tree', DecisionTreeClassifier(), {'max_depth': [None, 10, 20, 30]}),\n",
    "    ('Random Forest', RandomForestClassifier(), {'n_estimators': [50, 100, 150, 200]}),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(), {'n_estimators': [50, 100, 150, 200], 'learning_rate': [0.01, 0.1, 0.2]}),\n",
    "    ('Neural Network', MLPClassifier(), {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]}),\n",
    "    ('SVM', SVC(), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}),\n",
    "    ('Ridge Classifier', RidgeClassifier(), {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]})\n",
    "]\n",
    "\n",
    "# Loop through each model\n",
    "for name, model, params in models:\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f\"Test Set Accuracy: {test_accuracy}\")\n",
    "\n",
    "    print(\"\\n-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing KNN Classifier...\n",
      "Cross-Validation Scores: [0.89473684 0.73684211 0.75675676 0.75675676 0.75675676]\n",
      "Mean CV Score: 0.7803698435277383\n",
      "Best Hyperparameters: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "Test Set Accuracy: 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "print(\"Processing KNN Classifier...\")\n",
    "\n",
    "cv_scores = cross_val_score(knn_model, X_train.values, Y_train.values, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {np.mean(cv_scores)}\")\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on the test set using the best model\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "y_pred = best_knn_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Logistic Regression...\n",
      "Cross-Validation Scores: [0.81578947 0.65789474 0.81081081 0.78378378 0.83783784]\n",
      "Mean CV Score: 0.7812233285917497\n",
      "Best Parameters: {'C': 0.01}\n",
      "Test Set Accuracy: 0.7142857142857143\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Naive Bayes...\n",
      "Cross-Validation Scores: [0.78947368 0.73684211 0.78378378 0.75675676 0.83783784]\n",
      "Mean CV Score: 0.7809388335704124\n",
      "Best Parameters: {}\n",
      "Test Set Accuracy: 0.7619047619047619\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Decision Tree...\n",
      "Cross-Validation Scores: [0.89473684 0.78947368 0.78378378 0.72972973 0.83783784]\n",
      "Mean CV Score: 0.8071123755334282\n",
      "Best Parameters: {'max_depth': None}\n",
      "Test Set Accuracy: 0.8571428571428571\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Random Forest...\n",
      "Cross-Validation Scores: [0.86842105 0.78947368 0.83783784 0.83783784 0.89189189]\n",
      "Mean CV Score: 0.8450924608819346\n",
      "Best Parameters: {'n_estimators': 150}\n",
      "Test Set Accuracy: 0.8095238095238095\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Gradient Boosting...\n",
      "Cross-Validation Scores: [0.92105263 0.84210526 0.78378378 0.86486486 0.86486486]\n",
      "Mean CV Score: 0.8553342816500711\n",
      "Best Parameters: {'learning_rate': 0.2, 'n_estimators': 50}\n",
      "Test Set Accuracy: 0.8571428571428571\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Neural Network...\n",
      "Cross-Validation Scores: [0.86842105 0.89473684 0.83783784 0.86486486 0.94594595]\n",
      "Mean CV Score: 0.8823613086770982\n",
      "Best Parameters: {'alpha': 0.001, 'hidden_layer_sizes': (50, 50)}\n",
      "Test Set Accuracy: 0.8095238095238095\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing SVM...\n",
      "Cross-Validation Scores: [0.89473684 0.73684211 0.83783784 0.83783784 0.83783784]\n",
      "Mean CV Score: 0.8290184921763869\n",
      "Best Parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Test Set Accuracy: 0.8095238095238095\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Processing Ridge Classifier...\n",
      "Cross-Validation Scores: [0.76315789 0.76315789 0.81081081 0.75675676 0.86486486]\n",
      "Mean CV Score: 0.7917496443812233\n",
      "Best Parameters: {'alpha': 100}\n",
      "Test Set Accuracy: 0.7142857142857143\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 10, 100]}),\n",
    "    ('Naive Bayes', GaussianNB(), {}),\n",
    "    ('Decision Tree', DecisionTreeClassifier(), {'max_depth': [None, 10, 20, 30]}),\n",
    "    ('Random Forest', RandomForestClassifier(), {'n_estimators': [50, 100, 150, 200]}),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(), {'n_estimators': [50, 100, 150, 200], 'learning_rate': [0.01, 0.1, 0.2]}),\n",
    "    ('Neural Network', MLPClassifier(), {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]}),\n",
    "    ('SVM', SVC(), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}),\n",
    "    ('Ridge Classifier', RidgeClassifier(), {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]})\n",
    "]\n",
    "\n",
    "# Loop through each model\n",
    "for name, model, params in models:\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_pca, Y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train_pca, Y_train)\n",
    "\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_pca)\n",
    "    test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f\"Test Set Accuracy: {test_accuracy}\")\n",
    "\n",
    "    print(\"\\n-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing KNN Classifier...\n",
      "Cross-Validation Scores: [0.94736842 0.76315789 0.83783784 0.86486486 0.83783784]\n",
      "Mean CV Score: 0.8502133712660029\n",
      "Best Hyperparameters: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "Test Set Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "print(\"Processing KNN Classifier...\")\n",
    "\n",
    "cv_scores = cross_val_score(knn_model, X_train_pca, Y_train.values, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {np.mean(cv_scores)}\")\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on the test set using the best model\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "y_pred = best_knn_model.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
